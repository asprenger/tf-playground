{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In TensorFlow, you have to create a graph and run it within a session in order to execute the operations of the graph. Eager Execution (EE) enables you to run operations directly and inspect the output as the operations are executed.\n",
    "\n",
    "EE is greate for prototyping, is pythonic and intergrates pretty well with Numpy so it makes programming really easy and flexible. TensorFlow 2.0 will enable EE by default.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable eager execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing Ops Eagerly\n",
    "By perfoming operations you can see the output directly without creating a session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[4.]], shape=(1, 1), dtype=float32)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "x = [[2.]]\n",
    "m = tf.square(x)\n",
    "print(m)\n",
    "\n",
    "# Convert tensor to Numpy array\n",
    "print(type(m.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute an operation including two tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8  9]\n",
      " [18 19]]\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([[1, 2],\n",
    "                 [3, 4]])\n",
    "\n",
    "b = tf.constant([[2, 1],\n",
    "                 [3, 4]])\n",
    "\n",
    "ab = tf.matmul(a, b)\n",
    "\n",
    "print(ab.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants and Variables\n",
    " * tf.constant, creates a constant tensor populated with the values as argument. The values are immutable.\n",
    " * tf.Variable, this method encapsultes a mutable tensor that can be changed later using assign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[2 3]], shape=(1, 2), dtype=int32)\n",
      "Exception raised \n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([[2,3]])\n",
    "print(a)\n",
    "\n",
    "# A constant is immutable, you cannot assign a new value to it\n",
    "try:\n",
    "  a.assign([[3,4]])\n",
    "except:\n",
    "  print('Exception raised ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old value for v = 5.0\n",
      "New value for v = 2.0\n"
     ]
    }
   ],
   "source": [
    "# Variables are mutable and can be assigned a new value\n",
    "v = tf.Variable(5.)\n",
    "\n",
    "print('Old value for v =', v.numpy())\n",
    "v.assign(2.)\n",
    "print('New value for v =', v.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can increment/decrement the value of a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value     :  2.0\n",
      "increment :  3.0\n",
      "decrement :  2.0\n"
     ]
    }
   ],
   "source": [
    "v.assign(2.)\n",
    "print('value     : ', v.numpy())\n",
    "print('increment : ', tf.assign_add(v, 1).numpy())\n",
    "print('decrement : ', tf.assign_sub(v, 1).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tensor has name, type, shape and device properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name  :  Variable:0\n",
      "type  :  <dtype: 'float32'>\n",
      "shape :  ()\n",
      "device:  /job:localhost/replica:0/task:0/device:CPU:0\n"
     ]
    }
   ],
   "source": [
    "print('name  : ', v.name)\n",
    "print('type  : ', v.dtype)\n",
    "print('shape : ', v.shape)\n",
    "print('device: ', v.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors defined with eager execution are not attached to the (default) Graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for op in tf.get_default_graph().get_operations():\n",
    "    print(op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Evaluation\n",
    "You can use tf.GradientTape() method to record the gradient of an arbitrary function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/asprenger/anaconda3-2018.12/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:642: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "The gradient of loss at 2.0 is 6.0\n"
     ]
    }
   ],
   "source": [
    "w = tf.Variable(2.0)\n",
    "\n",
    "#watch the gradient of the loss operation\n",
    "with tf.GradientTape() as tape:\n",
    "  loss = 0.5 * w**3\n",
    "\n",
    "grad = tape.gradient(loss, w)\n",
    "print(f'The gradient of loss at {w.numpy()} is {grad.numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You also compute the gradient directly using `gradients_function`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow.contrib.eager as tfe \n",
    "\n",
    "def loss(w):\n",
    "    return 0.5 * w**3\n",
    "grad_loss = tfe.gradients_function(loss)\n",
    "grad_loss(2.0)[0].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we evaluate the gradient of the sigmoid function:\n",
    "\n",
    "$$f(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "$$f'(x) = \\frac{e^{-x}}{(1+e^{-x})^2} = f(x)(1-f(x)) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradient of the sigmoid function at 2.0 is  0.104993574\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + tf.exp(-x))\n",
    "\n",
    "grad_sigmoid = tfe.gradients_function(sigmoid)\n",
    "\n",
    "print('The gradient of the sigmoid function at 2.0 is ', grad_sigmoid(2.0)[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Higher order derivatives can be computed by nesting gradient functions:\n",
    "\n",
    "$$f(x) = \\log(x) , f'(x) = \\frac{1}{x}, f''(x) = \\frac{-1}{x^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first  derivative of log at x = 1 is  1.0\n",
      "The second derivative of log at x = 1 is  -1.0\n",
      "The third  derivative of log at x = 1 is  2.0\n"
     ]
    }
   ],
   "source": [
    "dx = tfe.gradients_function\n",
    "\n",
    "def log(x):\n",
    "    return tf.log(x)\n",
    "\n",
    "dx_log = dx(log)\n",
    "dx2_log = dx(dx(log))\n",
    "dx3_log = dx(dx(dx(log)))\n",
    "\n",
    "print('The first  derivative of log at x = 1 is ', dx_log(1.)[0].numpy())\n",
    "print('The second derivative of log at x = 1 is ', dx2_log(1.)[0].numpy())\n",
    "print('The third  derivative of log at x = 1 is ', dx3_log(1.)[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Gradients\n",
    "\n",
    "Some times the gradient is not what we want especially if there is a problem in numerical instabilitiy. Consider the following function and its gradient \n",
    "\n",
    "$$f(x) = \\log(1+e^x)$$\n",
    "\n",
    "The gradient is \n",
    "\n",
    "$$f'(x) = \\frac{e^x}{1+e^x}$$\n",
    "\n",
    "Note that at big values of $x$ the gradient value will blow up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradient at x = 0  is  0.5\n",
      "The gradient at x = 100 is  nan\n"
     ]
    }
   ],
   "source": [
    "def logexp(x):\n",
    "    return tf.log(1 + tf.exp(x))\n",
    "grad_logexp = tfe.gradients_function(logexp)\n",
    "\n",
    "print('The gradient at x = 0  is ', grad_logexp(0.)[0].numpy())  \n",
    "\n",
    "print('The gradient at x = 100 is ', grad_logexp(100.)[0].numpy()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can revaluate the gradient by overriding the gradient of the function. We can recompute the gradient as \n",
    "\n",
    "$$f(x) =  \\frac{1+e^x -e^x }{1+e^x} = 1 - \\frac{1}{1 + e^{x}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradient at x = 100 is  1.0\n"
     ]
    }
   ],
   "source": [
    "@tf.custom_gradient\n",
    "def logexp_stable(x):\n",
    "    e = tf.exp(x)\n",
    "    #dy is optional, allows computation of vector jacobian products for vectors other than the vector of ones.\n",
    "    def grad(dy):\n",
    "        return dy * (1 - 1 / (1 + e))\n",
    "    return tf.log(1 + e), grad\n",
    "\n",
    "grad_logexp_stable = tfe.gradients_function(logexp_stable)\n",
    "\n",
    "print('The gradient at x = 100 is ', grad_logexp_stable(100.)[0].numpy()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution callbacks\n",
    "\n",
    "`add_execution_callback` can be used to monitor the execution of operations. These functions will be called when any function is executed eagerly. In this example we record the operation names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pow\n",
      "Sub\n"
     ]
    }
   ],
   "source": [
    "# create a callback that records the operation name \n",
    "def print_op(op_type, op_name, attrs, inputs, outputs):\n",
    "    print(op_type)\n",
    "\n",
    "tfe.clear_execution_callbacks() \n",
    "\n",
    "# add a callback \n",
    "tfe.add_execution_callback(print_op)\n",
    "\n",
    "# run the operation\n",
    "x = tf.pow(2.0, 3.0) - 3.0\n",
    "\n",
    "tfe.clear_execution_callbacks() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression \n",
    "\n",
    "This example is refactored from https://www.tensorflow.org/guide/eager. We create a complete example of using linear regression to predict the paramters of the function \n",
    "\n",
    "$$f(x) = 3 x + 2 + noise$$\n",
    "\n",
    "Given a point $x$ we want to predict the value of $y$. We train the model on 1000 data pairs $(x,f(x))$. \n",
    "\n",
    "The model to learn is a linear model \n",
    "\n",
    "$$\\hat{y} = W x + b$$\n",
    "\n",
    "Note that, we use `tf.GradientTape` to record the gradient with respect our trainable paramters.  \n",
    "\n",
    "We MSE to calcuate the loss \n",
    "\n",
    "$$g = (y-\\hat{y})^2$$\n",
    "\n",
    "We use Gradient Descent to update the paramters \n",
    "\n",
    "$$W = W - \\alpha  \\frac{\\partial g}{\\partial W}$$\n",
    "\n",
    "$$b = b - \\alpha  \\frac{\\partial g}{\\partial b}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 000: 13.434\n",
      "Loss at step 020: 6.684\n",
      "Loss at step 040: 3.593\n",
      "Loss at step 060: 2.178\n",
      "Loss at step 080: 1.529\n",
      "Loss at step 100: 1.232\n",
      "Loss at step 120: 1.096\n",
      "Loss at step 140: 1.034\n",
      "Loss at step 160: 1.006\n",
      "Loss at step 180: 0.993\n",
      "W : 2.9493610858917236 , b  = 1.9152379035949707 \n"
     ]
    }
   ],
   "source": [
    "#1000 data points \n",
    "NUM_EXAMPLES = 1000\n",
    "\n",
    "#define inputs and outputs with some noise \n",
    "X = tf.random_normal([NUM_EXAMPLES])  #inputs \n",
    "noise = tf.random_normal([NUM_EXAMPLES]) #noise \n",
    "y = X * 3 + 2 + noise  #true output\n",
    "\n",
    "#create model paramters with initial values \n",
    "W = tf.Variable(0.)\n",
    "b = tf.Variable(0.)\n",
    "\n",
    "#training info\n",
    "train_steps = 200\n",
    "learning_rate = 0.01\n",
    "\n",
    "for i in range(train_steps):\n",
    "  \n",
    "    #watch the gradient flow \n",
    "    with tf.GradientTape() as tape:\n",
    "    \n",
    "        #forward pass \n",
    "        yhat = X * W + b\n",
    "\n",
    "        #calcuate the loss (difference squared error)\n",
    "        error = yhat - y\n",
    "        loss = tf.reduce_mean(tf.square(error))\n",
    "  \n",
    "    #evalute the gradient with the respect to the paramters\n",
    "    dW, db = tape.gradient(loss, [W, b])\n",
    "\n",
    "    #update the paramters using Gradient Descent  \n",
    "    W.assign_sub(dW * learning_rate)\n",
    "    b.assign_sub(db* learning_rate)\n",
    "\n",
    "    #print the loss every 20 iterations \n",
    "    if i % 20 == 0:\n",
    "        print(\"Loss at step {:03d}: {:.3f}\".format(i, loss))\n",
    "      \n",
    "print(f'W : {W.numpy()} , b  = {b.numpy()} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple CNN model\n",
    "Build the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/asprenger/anaconda3-2018.12/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 16)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 14, 14, 16)        64        \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 7, 7, 32)          128       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1568)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               156900    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 162,902\n",
      "Trainable params: 162,806\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, Convolution2D, MaxPooling2D, Flatten, BatchNormalization, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(filters = 16, kernel_size = 3, padding = 'same', input_shape = [28, 28, 1], activation = 'relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Convolution2D(filters = 32, kernel_size = 3, padding = 'same', activation = 'relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units = 100, activation = 'relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(units = 10 , activation = 'softmax'))\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the model to generate predictions on some fake data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "       [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "       [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "       [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "       [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "preds = model(np.zeros((5, 28, 28, 1), np.float32))\n",
    "preds.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The label is  5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "print('The label is ',y_train[0])\n",
    "plt.imshow(x_train[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "N = x_train.shape[0]\n",
    "\n",
    "# normalization and add channel dimension\n",
    "x_train = tf.expand_dims(np.float32(x_train)/ 255., 3)\n",
    "x_test  = tf.expand_dims(np.float32(x_test )/ 255., 3)\n",
    "\n",
    "# one hot encoding\n",
    "y_train = tf.one_hot(y_train, 10)\n",
    "y_test  = tf.one_hot(y_test , 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_batch(batch_size = 32):\n",
    "    r = np.random.randint(0, N-batch_size)\n",
    "    return x_train[r: r + batch_size], y_train[r: r + batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define loss, gradient and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the loss\n",
    "def loss(model, x, y):\n",
    "    prediction = model(x)\n",
    "    return tf.losses.softmax_cross_entropy(y, logits=prediction)\n",
    "\n",
    "# record the gradient with respect to the model variables \n",
    "def grad(model, x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model, x, y)\n",
    "    return tape.gradient(loss_value, model.variables)\n",
    "\n",
    "# calcuate the accuracy of the model \n",
    "def accuracy(model, x, y):\n",
    "  \n",
    "    # prediction\n",
    "    yhat = model(x)\n",
    "  \n",
    "    # get the labels of the predicted values \n",
    "    yhat = tf.argmax(yhat, 1).numpy()\n",
    "  \n",
    "    # get the labels of the true values\n",
    "    y    = tf.argmax(y   , 1).numpy()\n",
    "    return np.sum(y == yhat)/len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1 \n",
    "\n",
    "batch_size = 64 \n",
    "epoch_length = N // batch_size\n",
    "epoch = 0 \n",
    "epochs = 5 \n",
    "\n",
    "#use Adam optimizer \n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "#record epoch loss and accuracy  \n",
    "loss_history = tfe.metrics.Mean(\"loss\")\n",
    "accuracy_history = tfe.metrics.Mean(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/asprenger/anaconda3-2018.12/lib/python3.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "epoch: 0 Loss: 1.541, Acc: 0.929\n",
      "epoch: 1 Loss: 1.484, Acc: 0.979\n",
      "epoch: 2 Loss: 1.476, Acc: 0.986\n",
      "epoch: 3 Loss: 1.472, Acc: 0.990\n",
      "epoch: 4 Loss: 1.471, Acc: 0.991\n"
     ]
    }
   ],
   "source": [
    "while epoch < epochs:\n",
    "  # get next batch\n",
    "  x, y = get_batch(batch_size = batch_size)\n",
    "\n",
    "  # Calculate derivatives of the input function with respect to its parameters.\n",
    "  grads = grad(model, x, y)\n",
    "\n",
    "  # Apply the gradient to the model\n",
    "  optimizer.apply_gradients(zip(grads, model.variables),\n",
    "                            global_step=tf.train.get_or_create_global_step())\n",
    "  \n",
    "  #record the current loss and accuracy   \n",
    "  loss_history(loss(model, x, y))\n",
    "  accuracy_history(accuracy(model, x, y))\n",
    "  \n",
    "  if i % epoch_length == 0:\n",
    "    print(\"epoch: {:d} Loss: {:.3f}, Acc: {:.3f}\".format(epoch, loss_history.result(), accuracy_history.result()))\n",
    "    \n",
    "    #clear the history \n",
    "    loss_history.init_variables()\n",
    "    accuracy_history.init_variables()\n",
    "    \n",
    "    epoch += 1\n",
    "    \n",
    "  i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a directory for saving the model\n",
    "import os \n",
    "checkpoint_dir = 'model'\n",
    "os.mkdir(checkpoint_dir)\n",
    "\n",
    "# create a root for the checkpoint\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "root = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                           model=model,\n",
    "                           optimizer_step=tf.train.get_or_create_global_step())\n",
    "\n",
    "# save the model \n",
    "root.save(file_prefix=checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restore model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an empty model \n",
    "model = create_model()\n",
    "\n",
    "#accuracy of the empty model \n",
    "print('accuracy before retrieving the model ',accuracy(model, x_test, y_test))\n",
    "\n",
    "#create a checkpoint variable \n",
    "root = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                           model=create_model(),\n",
    "                           optimizer_step=tf.train.get_or_create_global_step())\n",
    "\n",
    "#restore the model\n",
    "root.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "#retrieve the trained model \n",
    "model = root.model \n",
    "\n",
    "print('accuracy after retrieving the model ',accuracy(model, x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling Functions into Callable Graph\n",
    "`defun` trace-compiles a Python function composed of TensorFlow operations into a callable that executes a tf.Graph containing those operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple example.\n",
    "def f(x):\n",
    "    return tf.square(x)\n",
    "\n",
    "# callable graph function\n",
    "g = tfe.defun(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant(3.)\n",
    "g(x).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can can use the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95257413"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tf.contrib.eager.defun\n",
    "def s(x):\n",
    "    return 1/(1+tf.exp(-x))\n",
    "\n",
    "s(x).numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
