{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look into tf.data\n",
    "Look at TF traceviewer (available outside TPU?)\n",
    "Goal is to get from IO bound to CPU xor better GPU bound (GPU is the most expensive device)\n",
    "look at Derek Murrays tf.data talk\n",
    "accelerator: GPU, TPU etc.\n",
    "Goal is to pipeline loading (disk), preprocessing (cpu) and loading data into GPU (PCI?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet-50: simple input pipeline\n",
    "\n",
    "def input_fn(batch_size, data_dir):\n",
    "    files = tf.data.Dataset.list_files(data_dir)\n",
    "    dataset = tf.data.TFRecordDataset(files, num_parallel_read=32)\n",
    "    # num_parallel_read: TFRecordDataset uses tf.contrib.data.parallel_interleave under the hood\n",
    "    dataset = dataset.shuffle(2048) # sliding window of 2048 records\n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "    dataset = dataset.map(parser_fn, num_parallel_calls=64)\n",
    "    # parser_fn:\n",
    "    # 1. parse TF.Example\n",
    "    # 2. image decode\n",
    "    # 3. random crop and flip\n",
    "    # 4. color transformations\n",
    "    #\n",
    "    # num_parallel_calls: paralleize across multiple cores\n",
    "    \n",
    "    dataset = dataset.batch(batch_size)\n",
    "    \n",
    "    dataset = dataset.prefetch(2)\n",
    "    # prefetch makes sure everything above is pipelined with the GPU training\n",
    "    # larger prefetch buffers also smooth over latency variability\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced techniques \n",
    " * use fused dataset operators\n",
    "   * map-and-batch\n",
    "   * shuffle-and-repeat\n",
    " * enable sloppy interleave\n",
    " * adjust read buffer sizes\n",
    " * perform transformation on accelerators\n",
    " \n",
    " TODO include optimized-input-pipeline image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use more compact formats:\n",
    "most projects use:\n",
    "fp32: single precision IEEE floating point format\n",
    "\n",
    "alternative:\n",
    "fp16: half-precision IEEE floating poin format\n",
    "\n",
    "More compact format allows bigger model on the GPU. Also GPUs are usually IO-bound and not compute bound, here smaller datasize helps as well.\n",
    "\n",
    "Look at bfloat16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hardware developments:\n",
    "NVIDIA volta's tensor cores have hardware accelerated matrix operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image layout affects performance:\n",
    " * channel last (NHWC)\n",
    " * channel first (NCHW) - FASTER!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For best performance use latest TF and driver versions\n",
    "Look at TensorRT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: include micro-benchmarks image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: look at XLA: xcelerated linear algebra"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
