{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A collection of example how to use the [Dataset API](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Dataset object from a numpy array using `from_tensor_slices()`. Call `make_one_shot_iterator()` on the dataset to create an iterator. This iterator can only be used once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "End of sequence\n"
     ]
    }
   ],
   "source": [
    "ds = tf.data.Dataset.from_tensor_slices(np.arange(0, 5))\n",
    "iterator = ds.make_one_shot_iterator()\n",
    "next_op = iterator.get_next()\n",
    "with tf.Session() as sess:\n",
    "    try:\n",
    "        while True:\n",
    "            val = sess.run(next_op)\n",
    "            print(val)\n",
    "    except tf.errors.OutOfRangeError as e:\n",
    "        print('End of sequence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call `make_initializable_iterator()` on the dataset to create an iterator that can be used multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0\n",
      "1: 1\n",
      "2: 2\n",
      "3: 3\n",
      "4: 4\n",
      "5: 0\n",
      "6: 1\n",
      "7: 2\n",
      "8: 3\n",
      "9: 4\n",
      "10: 0\n",
      "11: 1\n"
     ]
    }
   ],
   "source": [
    "ds = tf.data.Dataset.from_tensor_slices(np.arange(0, 5))\n",
    "iterator = ds.make_initializable_iterator()\n",
    "next_op = iterator.get_next()\n",
    "with tf.Session() as sess:\n",
    "    for i in range(12):\n",
    "        if i%5==0:\n",
    "            sess.run(iterator.initializer)\n",
    "        val = sess.run(next_op)\n",
    "        print('%d: %d' % (i, val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dataset can be repeated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "End of sequence\n"
     ]
    }
   ],
   "source": [
    "ds = tf.data.Dataset.from_tensor_slices(np.arange(0, 5)).repeat(2)\n",
    "iterator = ds.make_one_shot_iterator()\n",
    "next_op = iterator.get_next()\n",
    "with tf.Session() as sess:\n",
    "    try:\n",
    "        while True:\n",
    "            val = sess.run(next_op)\n",
    "            print(val)\n",
    "    except tf.errors.OutOfRangeError as e:\n",
    "        print('End of sequence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Dataset can batch samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "[2 3]\n",
      "[4]\n",
      "End of sequence\n"
     ]
    }
   ],
   "source": [
    "ds = tf.data.Dataset.from_tensor_slices(np.arange(0, 5)).batch(2)\n",
    "iterator = ds.make_one_shot_iterator()\n",
    "next_op = iterator.get_next()\n",
    "with tf.Session() as sess:\n",
    "    try:\n",
    "        while True:\n",
    "            val = sess.run(next_op)\n",
    "            print(val)\n",
    "    except tf.errors.OutOfRangeError as e:\n",
    "        print('End of sequence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets can be zipped together. This is useful to create pairs of training samples and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 0]), 0)\n",
      "(array([1, 1]), 1)\n",
      "(array([2, 2]), 1)\n",
      "(array([3, 3]), 0)\n",
      "(array([4, 4]), 0)\n",
      "End of sequence\n"
     ]
    }
   ],
   "source": [
    "X = tf.data.Dataset.from_tensor_slices(np.array([[0,0], [1,1], [2,2], [3,3], [4,4]]))\n",
    "y = tf.data.Dataset.from_tensor_slices(np.array([0,1,1,0,0]))\n",
    "ds = tf.data.Dataset.zip((X, y)) # try batching: .batch(2)\n",
    "iterator = ds.make_one_shot_iterator()\n",
    "next_op = iterator.get_next()\n",
    "with tf.Session() as sess:\n",
    "    try:\n",
    "        while True:\n",
    "            val = sess.run(next_op)\n",
    "            print(val)\n",
    "    except tf.errors.OutOfRangeError as e:\n",
    "        print('End of sequence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets can be split into a number of partitions by calling `shard()`. This is useful for distributed training or k-fold validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 0\n",
      "0\n",
      "4\n",
      "8\n",
      "12\n",
      "16\n",
      "Shard 1\n",
      "1\n",
      "5\n",
      "9\n",
      "13\n",
      "17\n",
      "Shard 2\n",
      "2\n",
      "6\n",
      "10\n",
      "14\n",
      "18\n",
      "Shard 3\n",
      "3\n",
      "7\n",
      "11\n",
      "15\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "ds = tf.data.Dataset.from_tensor_slices(np.arange(0, 20))\n",
    "num_shards = 4\n",
    "iterators = []\n",
    "for shard_idx in range(num_shards):\n",
    "    shard = ds.shard(num_shards, shard_idx)\n",
    "    iterator = shard.make_one_shot_iterator()\n",
    "    iterators.append(iterator)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for i, iterator in enumerate(iterators):\n",
    "        print('Shard %d' % i)\n",
    "        next_op = iterator.get_next()\n",
    "        try:\n",
    "            while True:\n",
    "                val = sess.run(next_op)\n",
    "                print(val)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The alternative to partition a Dataset is by using `skip()` and `take()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition 0\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "Partition 1\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Partition 2\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "Partition 3\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "ds = tf.data.Dataset.from_tensor_slices(np.arange(0, 20))\n",
    "num_partitions = 4\n",
    "iterators = []\n",
    "for i in range(num_partitions):\n",
    "    shard = ds.skip(i*5).take(5)\n",
    "    iterator = shard.make_one_shot_iterator()\n",
    "    iterators.append(iterator)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for i, iterator in enumerate(iterators):\n",
    "        print('Partition %d' % i)\n",
    "        next_op = iterator.get_next()\n",
    "        try:\n",
    "            while True:\n",
    "                val = sess.run(next_op)\n",
    "                print(val)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `map()` to transform a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "4\n",
      "9\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "ds = tf.data.Dataset.from_tensor_slices(np.arange(0, 5)).map(lambda x: x**2)\n",
    "iterator = ds.make_one_shot_iterator()\n",
    "next_op = iterator.get_next()\n",
    "with tf.Session() as sess:\n",
    "    try:\n",
    "        while True:\n",
    "            val = sess.run(next_op)\n",
    "            print(val)\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
