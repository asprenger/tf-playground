{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Links\n",
    "\n",
    "## Documentation, Blogs\n",
    "\n",
    " * [Official Distributed TensorFlow documentation](https://www.tensorflow.org/deploy/distributed)\n",
    "\n",
    " * [How to write distributed TensorFlow code — with an example on Clusterone](https://medium.com/clusterone/how-to-write-distributed-tensorflow-code-with-an-example-on-tensorport-70bf3306adcb)\n",
    "\n",
    " * [Oreilly blog post: Distributed TensorFlow](https://www.oreilly.com/ideas/distributed-tensorflow)\n",
    "\n",
    " * [Distributed Deep Learning - Part 1 - An Introduction](http://joerihermans.com/ramblings/distributed-deep-learning-part-1-an-introduction/)\n",
    "     * Introduction and description of several distributed training algorithms (e.g. how to aggregate gradient updates)\n",
    "\n",
    "## Code examples\n",
    "\n",
    " * [Official mnist example](https://github.com/tensorflow/models/blob/master/official/mnist/mnist.py)\n",
    "     * several interesting tricks\n",
    "     * non-distributed but has support for multiple GPUs \n",
    "\n",
    " * [Official cifar10 estimator example](https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10_estimator)\n",
    "     * several interesting tricks\n",
    "     * non-distributed but has support for multiple GPUs \n",
    "\n",
    " * [mnist_replica.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dist_test/python/mnist_replica.py)\n",
    "     * Distributed MNIST training and validation, with model replicas\n",
    "\n",
    " * [Multiple-GPUs-Single-Machine](https://github.com/tmulc18/Distributed-TensorFlow-Guide/tree/master/Multiple-GPUs-Single-Machine)\n",
    "     * Collection of distributed training examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategies for distributed learning\n",
    "\n",
    "## Model Parallelism\n",
    "\n",
    "Some models are so large they cannot fit in memory of a single GPU. Google’s Neural Machine Translation system is one example. Such models need to be split over many devices, carrying out the training in parallel on the devices. For example, different layers in a network may be trained in parallel on different GPUs. This training procedure is described as `in-graph replication` in the TensorFlow documentation.\n",
    "\n",
    "## Data Parallelism\n",
    "\n",
    "If the model can fit in the memory of a single GPU, we can store the model in every device, but train each model with different training samples. Each device will independently compute loss and gradients. The algorithm needs to combine the gradients and use the result to update the model.\n",
    "\n",
    "## Synchronous distributed training\n",
    "\n",
    "\n",
    "## Asynchronous distributed training\n",
    "\n",
    "    \n",
    "# Data parallelism in TensorFlow\n",
    "\n",
    "In the TensorFlow documentation data parallelism is called `between-graph replication`.\n",
    "    \n",
    "The entire graph will live on one machine called the **parameter server (ps)**. If the amount of I/O becomes to large for a single parameter server it can be replicated, a copy of the entire graph can live on several parameter servers that stay in sync.\n",
    "\n",
    "Training operations will be executed on multiple machines called **workers**. Each worker will be reading different data batches, computing gradients, and sending update to the parameter servers. Usually the workers will average there gradients and only a single update will be sent to the parameter server.\n",
    "\n",
    "  * **Synchronous training:** The worker will synchronize there work. At any point in time, two workers have the exact same graph parameters values.\n",
    "  * **Asynchronous training:** The workers will work asynchronously. At any point in time, two workers might have different graph parameters values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed TensorFlow\n",
    "\n",
    "Data-parallelism == between-graph replication\n",
    "\n",
    "TensorFlow has three types of nodes:\n",
    " * One or more **parameter servers** that host the graph\n",
    " * A **master worker** coordinates the training operations, and takes care of initializing the model, saving and restoring model checkpoints and saving summaries for TensorBoard. The master worker also takes care of fault-tolerance (if one ps or a worker crashes)\n",
    " * **workers** (including the master worker) handle compute training steps and send updates to the parameter servers\n",
    " \n",
    "**TODO:** How does the master worker handle worker crashes/recovery?\n",
    " \n",
    "The reason you might want to have more than one parameter server is to handle a large volume of I/O from the workers. \n",
    "\n",
    "Setting up distributed TensorFlow requires the following steps:\n",
    " * Define the `tf.trainClusterSpec` and `tf.train.Server`\n",
    " * Assign the graph to the parameter servers and workers\n",
    " * Configure and launch a `tf.train.MonitoredTrainingSession`\n",
    " \n",
    "A `tf.train.ClusterSpec` represents the set of processes that participate in a distributed TensorFlow computation.\n",
    "\n",
    "Every `tf.train.Server` belongs to a particular cluster. A `tf.train.Server` instance encapsulates a set of devices and corresponds to a particular task in a named job. The server can communicate with any other server in the same cluster.\n",
    "\n",
    "Using the `with tf.device` command, you can now assign nodes (either ops or variables) to a specific task of a specific job.\n",
    "\n",
    "In the data parallelism framework, variable operations will be assigned to parameter servers and training operations to workers.\n",
    "\n",
    "TensorFlow provides a convenient `tf.train.replica_device_setter` that automatically takes care of assigning operations to devices:\n",
    "\n",
    "\n",
    "    with tf.device(tf.train.replica_device_setter(cluster_spec)):\n",
    "        # define graph...\n",
    "        # define training operations...\n",
    "        \n",
    "`tf.train.MonitoredTrainingSession` is the equivalent of `tf.Session` for distributed training. It takes care of setting up a master worker node, that will handle:\n",
    "  * Initializing the graph\n",
    "  * Create checkpoints\n",
    "  * Exporting TensorBoard summaries\n",
    "  * Starting / stopping the session\n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "The parameter server combine the partial gradient updates.\n",
    "  \n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
